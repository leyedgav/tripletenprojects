{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Report\n",
    "\n",
    "### 1. Steps Performed and Steps Skipped  \n",
    "We carried out every major item in our original plan. We merged and cleaned all four source files on `customerID`, handled missing values (coercing blank `TotalCharges` to NaN and imputing with the median), converted dates and categorical fields, and encoded the target (`EndDate == 'No'`). We then performed EDA on feature distributions and confirmed that the target (`EndDate == 'No'` vs. other dates) was almost perfectly balanced at roughly 50/50, so we did not apply manual rebalancing. In feature engineering we derived tenure from the contract dates, encoded service options as binary flags, and scaled numeric fields. We trained logistic regression, Random Forest, and LightGBM models, and selected LightGBM as the best performer. \n",
    "\n",
    "### 2. Difficulties Encountered and Solutions  \n",
    "The most time-consuming task was merging disparate tables and aligning data types, particularly converting `TotalCharges` from string to numeric where some entries were blank. We solved this by coercing invalid entries to NaN and imputing with the column median. Running tree-based models on CPU required careful parameter tuning and early stopping, which LightGBM supports efficiently. Ensuring truly out-of-sample evaluation meant using a fixed random seed and a held-out validation split.\n",
    "\n",
    "### 3. Key Steps to Solving the Task  \n",
    "A. Data integration and cleaning to produce a single unified table.  \n",
    "B. Feature engineering—calculating customer tenure and encoding binary service features.  \n",
    "C. Model selection and tuning—benchmarking several algorithms and applying early stopping with LightGBM.  \n",
    "D. Rigorous evaluation—measuring AUC-ROC and accuracy on a held-out set, and verifying stability with ROC and Precision-Recall curves.\n",
    "\n",
    "### 4. Final Model and Quality Score  \n",
    "Our final solution is a LightGBM classifier trained on all engineered features with early stopping on the validation set. It achieved an **AUC-ROC of 0.87**, which according to the assessment criteria (0.87 ≤ AUC-ROC < 0.88) earns **5.5 story points**. The corresponding classification accuracy was **0.81** on the held-out validation data, confirming robust discrimination between churners and non-churners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Reviewer's comment</b> Nice writing and formatting here! Great job.\n",
    "<a class=\"tocSkip\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
