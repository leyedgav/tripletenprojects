{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<div style=\"border:solid green 2px; padding: 20px\">\n    \n<b>Hello, Edgardo!</b> We're glad to see you in code-reviewer territory. You've done a great job on the project, but let's get to know each other and make it even better! We have our own atmosphere here and a few rules:\n\n\n1. I work as a code reviewer, and my main goal is not to point out your mistakes, but to share my experience and help you become a data analyst.\n2. We speak on a first-come-first-served basis.\n3. if you want to write or ask a question, don't be shy. Just choose your color for your comment.  \n4. this is a training project, you don't have to be afraid of making a mistake.  \n5. You have an unlimited number of attempts to pass the project.  \n6. Let's Go!\n\n\n---\nI'll be color-coding comments, please don't delete them:\n\n<div class=\"alert alert-block alert-danger\">\u270d\n    \n\n__Reviewer's comment \u21161__\n\nNeeds fixing. The block requires some corrections. Work can't be accepted with the red comments.\n</div>\n    \n---\n\n<div class=\"alert alert-block alert-warning\">\ud83d\udcdd\n    \n\n__Reviewer's comment \u21161__\n\n\nRemarks. Some recommendations.\n</div>\n\n---\n\n<div class=\"alert alert-block alert-success\">\u2714\ufe0f\n    \n\n__Reviewer's comment \u21161__\n\nSuccess. Everything is done succesfully.\n</div>\n    \n---\n    \nI suggest that we work on the project in dialogue: if you change something in the project or respond to my comments, write about it. It will be easier for me to track changes if you highlight your comments:   \n    \n<div class=\"alert alert-info\"> <b>Student \u0441omments:</b> Student answer..</div>\n    \nAll this will help to make the recheck of your project faster. If you have any questions about my comments, let me know, we'll figure it out together :)   \n    \n---"}, {"cell_type": "markdown", "metadata": {}, "source": "In this project, we aim to assist Megaline, a mobile carrier, in optimizing their plan offerings by predicting the most suitable plan for their subscribers. With a dataset containing monthly behavioral information, such as call duration, number of messages, and internet usage, we seek to develop a machine learning model that can accurately recommend either the \"Smart\" or \"Ultra\" plan. The project's goal is to build a model with an accuracy of at least 0.75, ensuring reliable and data-driven plan recommendations. This analysis involves data preprocessing, model training, hyperparameter tuning, and evaluation to select the best-performing model for deployment."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\u2714\ufe0f\n    \n\n__Reviewer's comment \u21161__\n\nAn excellent practice is to describe the goal and main steps in your own words (a skill that will help a lot on a final project). "}, {"cell_type": "code", "execution_count": 1, "metadata": {"trusted": true}, "outputs": [], "source": "# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {"trusted": true}, "outputs": [{"data": {"text/plain": "(   calls  minutes  messages   mb_used  is_ultra\n 0   40.0   311.90      83.0  19915.42         0\n 1   85.0   516.75      56.0  22696.96         0\n 2   77.0   467.66      86.0  21060.45         0\n 3  106.0   745.53      81.0   8437.39         1\n 4   66.0   418.74       1.0  14502.75         0,\n calls       0\n minutes     0\n messages    0\n mb_used     0\n is_ultra    0\n dtype: int64,\n calls       float64\n minutes     float64\n messages    float64\n mb_used     float64\n is_ultra      int64\n dtype: object)"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "\n# Correct file path for the dataset\ndata_path = '/datasets/users_behavior.csv'\ndata = pd.read_csv(data_path)\n\n# Display the first few rows of the dataset to understand its structure\ndata_head = data.head()\n\n# Check for missing values and data types\nmissing_values = data.isnull().sum()\ndata_types = data.dtypes\n\ndata_head, missing_values, data_types\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Here, we load the dataset and conduct an initial inspection to understand its structure. We check for missing values and ensure that the data types are appropriate for the subsequent analysis."}, {"cell_type": "code", "execution_count": 3, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training set: 1928 samples\nValidation set: 643 samples\nTest set: 643 samples\n"}], "source": "\n# Split the data into features and target variable\nX = data.drop('is_ultra', axis=1)\ny = data['is_ultra']\n\n# Split the data into training, validation, and test sets (60%, 20%, 20%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Display the shapes of the resulting datasets\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\u2714\ufe0f\n    \n\n__Reviewer's comment \u21161__\n\n1. It is good here, random_state is fixed. We have ensured reproducibility of the results of splitting the sample into training (training) / test / validation samples, so the subsamples will be identical in all subsequent runs of our code.\n    \n2. Fraction of train/valid/test sizes 3:1:1 is good."}, {"cell_type": "markdown", "metadata": {}, "source": "The dataset is split into training, validation, and test sets. The training set is used for model training, the validation set for hyperparameter tuning, and the test set for evaluating the final model."}, {"cell_type": "code", "execution_count": 4, "metadata": {"trusted": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"}, {"name": "stdout", "output_type": "stream", "text": "Logistic Regression accuracy: 0.7403\nDecision Tree accuracy: 0.7278\nRandom Forest accuracy: 0.8025\nGradient Boosting accuracy: 0.8087\n"}], "source": "\n# Initialize the models\nlog_reg = LogisticRegression(random_state=42)\ndec_tree = DecisionTreeClassifier(random_state=42)\nrand_forest = RandomForestClassifier(random_state=42)\ngrad_boost = GradientBoostingClassifier(random_state=42)\n\n# Fit the models\nlog_reg.fit(X_train, y_train)\ndec_tree.fit(X_train, y_train)\nrand_forest.fit(X_train, y_train)\ngrad_boost.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred_log_reg = log_reg.predict(X_val)\ny_pred_dec_tree = dec_tree.predict(X_val)\ny_pred_rand_forest = rand_forest.predict(X_val)\ny_pred_grad_boost = grad_boost.predict(X_val)\n\n# Calculate accuracy\nacc_log_reg = accuracy_score(y_val, y_pred_log_reg)\nacc_dec_tree = accuracy_score(y_val, y_pred_dec_tree)\nacc_rand_forest = accuracy_score(y_val, y_pred_rand_forest)\nacc_grad_boost = accuracy_score(y_val, y_pred_grad_boost)\n\n# Display the accuracies\nprint(f\"Logistic Regression accuracy: {acc_log_reg:.4f}\")\nprint(f\"Decision Tree accuracy: {acc_dec_tree:.4f}\")\nprint(f\"Random Forest accuracy: {acc_rand_forest:.4f}\")\nprint(f\"Gradient Boosting accuracy: {acc_grad_boost:.4f}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Baseline models are trained using default hyperparameters. We evaluate these models' performance on the validation set to establish a benchmark accuracy."}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": "\n\n# Load the dataset\ndata = pd.read_csv('/datasets/users_behavior.csv')\n\n# Split the data into features and target variable\nX = data.drop('is_ultra', axis=1)\ny = data['is_ultra']\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define hyperparameters grid for Random Forest\nrf_params = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Define hyperparameters grid for Gradient Boosting\ngb_params = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 7]\n}\n\n# Initialize GridSearchCV for Random Forest\ngrid_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=rf_params, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_rf.fit(X_train, y_train)\n\n# Initialize GridSearchCV for Gradient Boosting\ngrid_gb = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=gb_params, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_gb.fit(X_train, y_train)\n\n# Best parameters and best accuracy for Random Forest\nbest_params_rf = grid_rf.best_params_\nbest_score_rf = grid_rf.best_score_\n\n# Best parameters and best accuracy for Gradient Boosting\nbest_params_gb = grid_gb.best_params_\nbest_score_gb = grid_gb.best_score_\n\nprint(f\"Random Forest best parameters: {best_params_rf}, best accuracy: {best_score_rf}\")\nprint(f\"Gradient Boosting best parameters: {best_params_gb}, best accuracy: {best_score_gb}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "We use GridSearchCV to tune hyperparameters for the Random Forest and Gradient Boosting models, aiming to optimize accuracy on the validation set. The best parameters and corresponding accuracy scores are recorded."}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": "\n# Initialize the models\nlog_reg = LogisticRegression(random_state=42)\ndec_tree = DecisionTreeClassifier(random_state=42)\nrand_forest = RandomForestClassifier(random_state=42)\ngrad_boost = GradientBoostingClassifier(random_state=42)\n\n# Fit the models\nlog_reg.fit(X_train, y_train)\ndec_tree.fit(X_train, y_train)\nrand_forest.fit(X_train, y_train)\ngrad_boost.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred_log_reg = log_reg.predict(X_val)\ny_pred_dec_tree = dec_tree.predict(X_val)\ny_pred_rand_forest = rand_forest.predict(X_val)\ny_pred_grad_boost = grad_boost.predict(X_val)\n\n# Calculate accuracy\nacc_log_reg = accuracy_score(y_val, y_pred_log_reg)\nacc_dec_tree = accuracy_score(y_val, y_pred_dec_tree)\nacc_rand_forest = accuracy_score(y_val, y_pred_rand_forest)\nacc_grad_boost = accuracy_score(y_val, y_pred_grad_boost)\n\n# Display the accuracies\nprint(f\"Logistic Regression accuracy: {acc_log_reg:.4f}\")\nprint(f\"Decision Tree accuracy: {acc_dec_tree:.4f}\")\nprint(f\"Random Forest accuracy: {acc_rand_forest:.4f}\")\nprint(f\"Gradient Boosting accuracy: {acc_grad_boost:.4f}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "We start by loading the dataset, which includes features (X) and the target variable (y), indicating the type of plan (0 for Smart, 1 for Ultra). The data is split into training (60%), validation, and test sets (20% each) to facilitate model training, hyperparameter tuning, and final evaluation. Four models\u2014Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting\u2014are initialized and trained on the training set. Their performance is assessed on the validation set, providing baseline accuracy scores for comparison."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\u2714\ufe0f\n    \n\n__Reviewer's comment \u21161__\n\nI agree"}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": "\n# Load the dataset\ndata_path = '/datasets/users_behavior.csv'\ndata = pd.read_csv(data_path)\n\n# Split the data into features and target variable\nX = data.drop('is_ultra', axis=1)\ny = data['is_ultra']\n\n# Split the data into training, validation, and test sets (60%, 20%, 20%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define hyperparameters grid for Random Forest\nrf_params = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Define hyperparameters grid for Gradient Boosting\ngb_params = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 7]\n}\n\n# Initialize GridSearchCV for Random Forest\ngrid_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=rf_params, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_rf.fit(X_train, y_train)\n\n# Initialize GridSearchCV for Gradient Boosting\ngrid_gb = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=gb_params, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_gb.fit(X_train, y_train)\n\n# Best parameters and best accuracy for Random Forest\nbest_params_rf = grid_rf.best_params_\nbest_score_rf = grid_rf.best_score_\n\n# Best parameters and best accuracy for Gradient Boosting\nbest_params_gb = grid_gb.best_params_\nbest_score_gb = grid_gb.best_score_\n\nbest_params_rf, best_score_rf, best_params_gb, best_score_gb\n"}, {"cell_type": "markdown", "metadata": {}, "source": "We perform hyperparameter tuning using GridSearchCV for the Random Forest and Gradient Boosting models. We define parameter grids for each model, including options for the number of trees, maximum depth, and other key settings. GridSearchCV uses 5-fold cross-validation to identify the best parameters based on accuracy. The optimal hyperparameters and their corresponding accuracy scores are then determined, helping us choose the best model configuration for accurate plan recommendations."}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": "# Train the best Random Forest model on the combined training and validation sets\nbest_rf = RandomForestClassifier(**best_params_rf, random_state=42)\nbest_rf.fit(X_train, y_train)\n\n# Evaluate the model on the test set\ny_test_pred = best_rf.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\nprint(f\"Test set accuracy: {test_accuracy:.4f}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "The final Random Forest model is trained using the best hyperparameters and evaluated on the test set. The test set accuracy provides an unbiased estimate of the model's performance."}, {"cell_type": "markdown", "metadata": {}, "source": "The project successfully developed a machine learning model to predict the optimal plan for Megaline's subscribers, achieving the target accuracy threshold. Through careful data preprocessing and exploration, we ensured the quality and readiness of the data for modeling. Initial model evaluations provided a baseline, while hyperparameter tuning optimized the model's performance. The final model, with tuned hyperparameters, demonstrated strong predictive capabilities on the test set, making it a valuable tool for guiding Megaline's marketing and customer service strategies. This model not only improves the user experience by recommending the most cost-effective plan but also aids Megaline in maximizing their customer retention and satisfaction efforts. Future work could focus on incorporating additional features, such as demographic data or usage trends, to further enhance the model's accuracy and applicability."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\u2714\ufe0f\n    \n\n__Reviewer's comment \u21161__\n\nHere's the great thing: we picked the best hyperparameters for all our models (in this case, maximizing the accuracy_score metric). Here we also identified the MOST optimal model. On validation, it turned out to be the \"random forest\" model.\n\nAfter the hyperparameters are selected for validation, we test the models on the test data. Based on the results of testing on the test (sorry for the tautalogy), we choose a model that we can pass to production."}], "metadata": {"ExecuteTimeLog": [{"duration": 309, "start_time": "2024-07-25T02:00:55.671Z"}, {"duration": 0, "start_time": "2024-07-25T02:00:55.983Z"}, {"duration": 0, "start_time": "2024-07-25T02:00:55.985Z"}, {"duration": 0, "start_time": "2024-07-25T02:00:55.988Z"}, {"duration": 24, "start_time": "2024-07-25T02:02:47.421Z"}, {"duration": 999, "start_time": "2024-07-25T02:02:50.164Z"}, {"duration": 25, "start_time": "2024-07-25T02:02:53.981Z"}, {"duration": 1030, "start_time": "2024-07-25T02:04:32.096Z"}, {"duration": 123, "start_time": "2024-07-25T02:05:04.122Z"}, {"duration": 123, "start_time": "2024-07-25T02:06:35.538Z"}, {"duration": 315, "start_time": "2024-07-25T02:06:49.104Z"}, {"duration": 0, "start_time": "2024-07-25T02:06:49.423Z"}, {"duration": 0, "start_time": "2024-07-25T02:06:49.424Z"}, {"duration": 0, "start_time": "2024-07-25T02:06:49.426Z"}, {"duration": 1472, "start_time": "2024-07-25T02:08:24.727Z"}, {"duration": 0, "start_time": "2024-07-25T02:08:26.202Z"}, {"duration": 0, "start_time": "2024-07-25T02:08:26.203Z"}, {"duration": 0, "start_time": "2024-07-25T02:08:26.204Z"}, {"duration": 32, "start_time": "2024-07-25T02:10:53.917Z"}, {"duration": 446, "start_time": "2024-07-25T02:11:08.976Z"}, {"duration": 705, "start_time": "2024-07-25T02:11:09.426Z"}, {"duration": 1014, "start_time": "2024-07-25T02:11:10.134Z"}, {"duration": 1336, "start_time": "2024-07-25T02:11:11.153Z"}, {"duration": 135, "start_time": "2024-07-25T02:11:40.023Z"}, {"duration": 484, "start_time": "2024-07-25T02:12:22.501Z"}, {"duration": 725, "start_time": "2024-07-25T02:12:22.989Z"}, {"duration": 1008, "start_time": "2024-07-25T02:12:23.718Z"}, {"duration": 283120, "start_time": "2024-07-25T02:12:24.730Z"}, {"duration": 967, "start_time": "2024-07-25T02:17:07.854Z"}, {"duration": 447, "start_time": "2024-07-25T02:19:08.873Z"}, {"duration": 695, "start_time": "2024-07-25T02:19:09.324Z"}, {"duration": 996, "start_time": "2024-07-25T02:19:10.024Z"}, {"duration": 5119, "start_time": "2024-07-25T02:22:31.367Z"}, {"duration": 31, "start_time": "2024-07-25T02:22:36.489Z"}, {"duration": 13, "start_time": "2024-07-25T02:22:36.523Z"}, {"duration": 968, "start_time": "2024-07-25T02:22:36.540Z"}, {"duration": 284373, "start_time": "2024-07-25T02:22:37.513Z"}, {"duration": 947, "start_time": "2024-07-25T02:27:21.890Z"}, {"duration": 1166, "start_time": "2024-07-25T02:27:22.840Z"}, {"duration": 0, "start_time": "2024-07-25T02:27:24.009Z"}, {"duration": 1571, "start_time": "2024-07-25T02:29:42.328Z"}, {"duration": 25, "start_time": "2024-07-25T02:29:43.902Z"}, {"duration": 13, "start_time": "2024-07-25T02:29:43.931Z"}, {"duration": 981, "start_time": "2024-07-25T02:29:43.947Z"}, {"duration": 1635, "start_time": "2024-07-25T02:32:50.614Z"}, {"duration": 23, "start_time": "2024-07-25T02:32:52.252Z"}, {"duration": 11, "start_time": "2024-07-25T02:32:52.278Z"}, {"duration": 955, "start_time": "2024-07-25T02:32:52.309Z"}, {"duration": 282438, "start_time": "2024-07-25T02:32:53.268Z"}, {"duration": 941, "start_time": "2024-07-25T02:37:35.709Z"}, {"duration": 283822, "start_time": "2024-07-25T02:37:36.653Z"}, {"duration": 1261, "start_time": "2024-07-25T02:42:20.478Z"}, {"duration": 5215, "start_time": "2024-08-05T18:12:06.230Z"}, {"duration": 28, "start_time": "2024-08-05T18:12:11.448Z"}, {"duration": 14, "start_time": "2024-08-05T18:12:11.478Z"}, {"duration": 979, "start_time": "2024-08-05T18:12:11.513Z"}], "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.19"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": true, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "275px"}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 2}