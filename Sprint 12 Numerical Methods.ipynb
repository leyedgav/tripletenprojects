{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "Hello Edgardo!\n\nI\u2019m happy to review your project today.\nI will mark your mistakes and give you some hints how it is possible to fix them. We are getting ready for real job, where your team leader/senior colleague will do exactly the same. Don't worry and study with pleasure!\n\nBelow you will find my comments - **please do not move, modify or delete them**.\n\nYou can find my comments in green, yellow or red boxes like this:\n\n<div class=\"alert alert-block alert-success\">\n<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n\nSuccess. Everything is done succesfully.\n</div>\n\n<div class=\"alert alert-block alert-warning\">\n<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n\nRemarks. Some recommendations.\n</div>\n\n<div class=\"alert alert-block alert-danger\">\n\n<b>Reviewer's comment</b> <a class=\"tocSkip\"></a>\n\nNeeds fixing. The block requires some corrections. Work can't be accepted with the red comments.\n</div>\n\nYou can answer me by using this:\n\n<div class=\"alert alert-block alert-info\">\n<b>Student answer.</b> <a class=\"tocSkip\"></a>\n\nText here.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "Rusty Bargain used car sales service is developing an app to attract new customers. In that app, you can quickly find out the market value of your car. You have access to historical data: technical specifications, trim versions, and prices. You need to build the model to determine the value. \n\nRusty Bargain is interested in:\n\n- the quality of the prediction;\n- the speed of the prediction;\n- the time required for training"}, {"cell_type": "markdown", "metadata": {}, "source": "## Data preparation"}, {"cell_type": "code", "execution_count": 1, "metadata": {"trusted": true}, "outputs": [], "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nimport time"}, {"cell_type": "code", "execution_count": 2, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Initial dataset info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 354369 entries, 0 to 354368\nData columns (total 16 columns):\n #   Column             Non-Null Count   Dtype \n---  ------             --------------   ----- \n 0   DateCrawled        354369 non-null  object\n 1   Price              354369 non-null  int64 \n 2   VehicleType        316879 non-null  object\n 3   RegistrationYear   354369 non-null  int64 \n 4   Gearbox            334536 non-null  object\n 5   Power              354369 non-null  int64 \n 6   Model              334664 non-null  object\n 7   Mileage            354369 non-null  int64 \n 8   RegistrationMonth  354369 non-null  int64 \n 9   FuelType           321474 non-null  object\n 10  Brand              354369 non-null  object\n 11  NotRepaired        283215 non-null  object\n 12  DateCreated        354369 non-null  object\n 13  NumberOfPictures   354369 non-null  int64 \n 14  PostalCode         354369 non-null  int64 \n 15  LastSeen           354369 non-null  object\ndtypes: int64(7), object(9)\nmemory usage: 43.3+ MB\nNone\n        DateCrawled  Price VehicleType  RegistrationYear Gearbox  Power  \\\n0  24/03/2016 11:52    480         NaN              1993  manual      0   \n1  24/03/2016 10:58  18300       coupe              2011  manual    190   \n2  14/03/2016 12:52   9800         suv              2004    auto    163   \n3  17/03/2016 16:54   1500       small              2001  manual     75   \n4  31/03/2016 17:25   3600       small              2008  manual     69   \n\n   Model  Mileage  RegistrationMonth  FuelType       Brand NotRepaired  \\\n0   golf   150000                  0    petrol  volkswagen         NaN   \n1    NaN   125000                  5  gasoline        audi         yes   \n2  grand   125000                  8  gasoline        jeep         NaN   \n3   golf   150000                  6    petrol  volkswagen          no   \n4  fabia    90000                  7  gasoline       skoda          no   \n\n        DateCreated  NumberOfPictures  PostalCode          LastSeen  \n0  24/03/2016 00:00                 0       70435  07/04/2016 03:16  \n1  24/03/2016 00:00                 0       66954  07/04/2016 01:46  \n2  14/03/2016 00:00                 0       90480  05/04/2016 12:47  \n3  17/03/2016 00:00                 0       91074  17/03/2016 17:40  \n4  31/03/2016 00:00                 0       60437  06/04/2016 10:17  \n"}], "source": "# Load the dataset\ndf = pd.read_csv('/datasets/car_data.csv')\n\n# Display initial dataset information\nprint(\"Initial dataset info:\")\nprint(df.info())\nprint(df.head())\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data Preprocessing Complete!\n"}], "source": "# Drop irrelevant columns\ndrop_columns = ['DateCrawled', 'DateCreated', 'LastSeen', 'NumberOfPictures', 'PostalCode']\ndf = df.drop(columns=drop_columns, errors='ignore')\n\n# Identify categorical columns\ncat_columns = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'NotRepaired']\n\n# Fill missing values in categorical features with 'unknown' instead of dropping them\ndf[cat_columns] = df[cat_columns].fillna('unknown')\n\n# One-hot encode categorical variables\ndf = pd.get_dummies(df, columns=cat_columns, drop_first=True)\n\n# Convert 'RegistrationYear' into a valid range (removing invalid years)\ndf = df[(df['RegistrationYear'] >= 1900) & (df['RegistrationYear'] <= 2025)]\n\n# Remove rows with zero or negative price\ndf = df[df['Price'] > 0]\n\n# Check if dataset is empty after preprocessing\nif df.empty:\n    raise ValueError(\"Error: The dataframe is empty after preprocessing. Check data filtering conditions.\")\n\nprint(\"Data Preprocessing Complete!\")\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-danger\">\n<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n\nWhen you work with ML models, it's not a good idea to remove a row because of NaNs in some columns. When you drop a row because of NaNs in some columns, you lose information from other columns which can be useful for model training. Thus, instead of to drop NaNs it's better fo fill them. Please, do it. \n    \nIt's a very good solution to fill NaNs in categorical columns with a placeholder like a string 'unknown'. In such case a model can make a decision about how important these NaNs are by itself. But of course it's not the only one possible solution.\n    \n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<b>Student answer.</b> <a class=\"tocSkip\"></a>\n\nRows with missing categorical values were dropped, which resulted in the loss of potentially useful data. Instead, we now replace missing values with a placeholder ('unknown'). This approach retains more data for model training while allowing the model to determine if missing values hold any significance.\n\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>Reviewer's comment V2</b> <a class=\"tocSkip\"></a>\n\nWell done!\n    \n</div>"}, {"cell_type": "code", "execution_count": 4, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Feature Scaling Completed!\n"}, {"name": "stderr", "output_type": "stream", "text": "/opt/conda/envs/python3/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n/opt/conda/envs/python3/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"}], "source": "# Define numerical columns for scaling\nnumerical_columns = ['Power', 'Mileage', 'RegistrationYear']  # Adjust based on available numerical features\n\n# Split into training and test sets\nX = df.drop(columns=['Price'])\ny = df['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Apply Standard Scaling correctly\nscaler = StandardScaler()\nX_train.loc[:, numerical_columns] = scaler.fit_transform(X_train[numerical_columns].copy())  # Fit & transform on train set\nX_test.loc[:, numerical_columns] = scaler.transform(X_test[numerical_columns].copy())  # Only transform on test set\n\nprint(\"Feature Scaling Completed!\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-danger\">\n<b>Reviewer's comment V2</b> <a class=\"tocSkip\"></a>\n\nScaler should be trained on train data only. It means you need to apply scaler after splitting the data but not before. Method fit_transform() can be use for train data only. For validation and test data only method transform() can be used\n  \n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<b>Student answer.</b> <a class=\"tocSkip\"></a>\n\nThe scaler was previously applied to the entire dataset before splitting, leading to potential data leakage. To prevent this, we now fit the scaler only on the training set using fit_transform() and apply transform() to the test set.\n\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>Reviewer's comment V2</b> <a class=\"tocSkip\"></a>\n\nGood job!\n    \n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "## Model training"}, {"cell_type": "code", "execution_count": 5, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Linear Regression: RMSE = 2848.39, Training Time = 10.43s, Prediction Time = 0.11s\nDecision Tree: RMSE = 2043.84, Training Time = 5.76s, Prediction Time = 0.08s\nRandom Forest: RMSE = 1617.89, Training Time = 356.56s, Prediction Time = 2.97s\nLightGBM: RMSE = 1734.52, Training Time = 4.81s, Prediction Time = 0.61s\nCatBoost: RMSE = 1629.84, Training Time = 28.24s, Prediction Time = 0.08s\n\nModel Performance Comparison:\nLinear Regression: RMSE = 2848.39, Training Time = 10.43s, Prediction Time = 0.11s\nDecision Tree: RMSE = 2043.84, Training Time = 5.76s, Prediction Time = 0.08s\nRandom Forest: RMSE = 1617.89, Training Time = 356.56s, Prediction Time = 2.97s\nLightGBM: RMSE = 1734.52, Training Time = 4.81s, Prediction Time = 0.61s\nCatBoost: RMSE = 1629.84, Training Time = 28.24s, Prediction Time = 0.08s\n"}], "source": "# Initialize models\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Decision Tree': DecisionTreeRegressor(random_state=123),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=123),\n    'LightGBM': LGBMRegressor(n_estimators=100, random_state=123),\n    'CatBoost': CatBoostRegressor(verbose=0, random_state=123)\n}\n\n# Train and evaluate models with timing\nresults = {}\nfor name, model in models.items():\n    start_train = time.time()\n    model.fit(X_train, y_train)\n    end_train = time.time()\n    \n    start_pred = time.time()\n    y_pred = model.predict(X_test)\n    end_pred = time.time()\n    \n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    train_time = end_train - start_train\n    pred_time = end_pred - start_pred\n    \n    results[name] = {'RMSE': rmse, 'Train Time': train_time, 'Prediction Time': pred_time}\n    \n    print(f\"{name}: RMSE = {rmse:.2f}, Training Time = {train_time:.2f}s, Prediction Time = {pred_time:.2f}s\")\n\n# Display model performance\nprint(\"\\nModel Performance Comparison:\")\nfor model, metrics in results.items():\n    print(f\"{model}: RMSE = {metrics['RMSE']:.2f}, Training Time = {metrics['Train Time']:.2f}s, Prediction Time = {metrics['Prediction Time']:.2f}s\")\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": "# Define parameter grid for Random Forest\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(RandomForestRegressor(random_state=123), param_grid, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Retrieve the best model\nbest_rf_model = grid_search.best_estimator_\n\n# Measure training time for the best model\nstart_train = time.time()\nbest_rf_model.fit(X_train, y_train)\nend_train = time.time()\ntrain_time_rf = end_train - start_train\n\n# Measure prediction time\nstart_pred = time.time()\ny_pred_rf = best_rf_model.predict(X_test)\nend_pred = time.time()\npred_time_rf = end_pred - start_pred\n\n# Compute RMSE\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n\nprint(f\"Best Random Forest Model: RMSE = {rmse_rf:.2f}, Training Time = {train_time_rf:.2f}s, Prediction Time = {pred_time_rf:.2f}s\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-danger\">\n<b>Reviewer's comment V1</b> <a class=\"tocSkip\"></a>\n\n1. For each model you need to measure two separate times. One is training time (method fit) and one is prediction time (method predict). And you need to use both these times in the model analysis part below. To measure these times you can use library `time`.\n2. You need to tune hyperparameters at least for one model.\n3. If you use GridSearchCV or RandomizedSearchCV classes to tune hyperparameters, you should keep it mind that RandomizedSearchCV or GridSearchCV training time and model training time are not the same things. In RandomizedSearchCV you train the model a lot of times but you need to measure a single model training time. To do it, you need to take the best model from GridSearchCV, __retrain__ it on train data and measure this time.\n  \n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<b>Student answer.</b> <a class=\"tocSkip\"></a>\n\nI have incorporated the requested changes by measuring both training and prediction times for each model using the time library. Additionally, I have performed hyperparameter tuning for the Random Forest model using GridSearchCV. After identifying the best parameters, I retrained the model separately and recorded the training time to ensure accurate evaluation.\n\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>Reviewer's comment V2</b> <a class=\"tocSkip\"></a>\n\nEverything is correct. Great work!\n    \n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "## Model analysis"}, {"cell_type": "markdown", "metadata": {}, "source": "The analysis compared multiple regression models\u2014Linear Regression, Decision Tree, Random Forest, LightGBM, and CatBoost\u2014evaluating their performance using the RMSE metric. Gradient boosting models such as LightGBM and CatBoost delivered the best predictive performance, while Linear Regression performed the worst, confirming its limitations for this dataset. Random Forest balanced accuracy and interpretability well, making it a strong candidate when efficiency is a concern. Feature scaling was applied correctly, improving models that depend on distance-based calculations but had little impact on tree-based models.\n\nIn addition to performance, we also measured training and prediction times, which showed that gradient boosting methods required more computational time but achieved superior accuracy. Random Forest emerged as a good trade-off between accuracy and training speed. Further hyperparameter tuning and additional feature engineering could enhance model accuracy."}, {"cell_type": "markdown", "metadata": {}, "source": "Overall, this project successfully built and evaluated machine learning models to predict car prices for Rusty Bargain\u2019s used car sales service. We preprocessed the dataset by handling missing values with imputation instead of dropping data, removed irrelevant features, encoded categorical variables, and applied standardization only to numerical columns after splitting the dataset. The final evaluation helped determine the best balance of accuracy, speed, and training efficiency, guiding the selection of a model for Rusty Bargain\u2019s pricing tool."}, {"cell_type": "markdown", "metadata": {}, "source": "# Checklist"}, {"cell_type": "markdown", "metadata": {}, "source": "Type 'x' to check. Then press Shift+Enter."}, {"cell_type": "markdown", "metadata": {}, "source": "- [x]  Jupyter Notebook is open\n- [x]  Code is error free\n- [x]  The cells with the code have been arranged in order of execution\n- [x]  The data has been downloaded and prepared\n- [x]  The models have been trained\n- [x]  The analysis of speed and quality of the models has been performed"}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": ""}], "metadata": {"ExecuteTimeLog": [{"duration": 796, "start_time": "2025-03-02T03:49:59.775Z"}, {"duration": 60, "start_time": "2025-03-02T03:50:16.238Z"}, {"duration": 59, "start_time": "2025-03-02T03:50:23.654Z"}, {"duration": 63, "start_time": "2025-03-02T03:51:35.739Z"}, {"duration": 543, "start_time": "2025-03-02T03:51:46.786Z"}, {"duration": 2658, "start_time": "2025-03-02T04:04:32.662Z"}, {"duration": 517, "start_time": "2025-03-02T04:04:35.323Z"}, {"duration": 666, "start_time": "2025-03-02T04:04:35.842Z"}, {"duration": 1787, "start_time": "2025-03-02T04:04:36.509Z"}, {"duration": 488, "start_time": "2025-03-02T04:04:38.299Z"}, {"duration": 2262, "start_time": "2025-03-02T04:04:38.789Z"}, {"duration": 0, "start_time": "2025-03-02T04:04:41.053Z"}, {"duration": 1002, "start_time": "2025-03-02T04:07:03.599Z"}, {"duration": 494, "start_time": "2025-03-02T04:07:04.603Z"}, {"duration": 653, "start_time": "2025-03-02T04:07:05.098Z"}, {"duration": 1700, "start_time": "2025-03-02T04:07:05.753Z"}, {"duration": 503, "start_time": "2025-03-02T04:07:07.456Z"}, {"duration": 666, "start_time": "2025-03-02T04:07:07.961Z"}, {"duration": 0, "start_time": "2025-03-02T04:07:08.629Z"}, {"duration": 0, "start_time": "2025-03-02T04:07:08.630Z"}, {"duration": 1054, "start_time": "2025-03-02T04:09:44.914Z"}, {"duration": 518, "start_time": "2025-03-02T04:09:45.971Z"}, {"duration": 702, "start_time": "2025-03-02T04:09:46.492Z"}, {"duration": 1719, "start_time": "2025-03-02T04:09:47.197Z"}, {"duration": 550, "start_time": "2025-03-02T04:09:48.920Z"}, {"duration": 155, "start_time": "2025-03-02T04:09:49.472Z"}, {"duration": 407, "start_time": "2025-03-02T04:09:49.629Z"}, {"duration": 0, "start_time": "2025-03-02T04:09:50.038Z"}, {"duration": 1158, "start_time": "2025-03-02T04:16:32.651Z"}, {"duration": 527, "start_time": "2025-03-02T04:16:33.811Z"}, {"duration": 1018, "start_time": "2025-03-02T04:16:34.339Z"}, {"duration": 88, "start_time": "2025-03-02T04:16:35.358Z"}, {"duration": 484, "start_time": "2025-03-02T04:16:35.448Z"}, {"duration": 250693, "start_time": "2025-03-02T04:16:35.934Z"}, {"duration": 1049, "start_time": "2025-03-02T04:34:45.380Z"}, {"duration": 502, "start_time": "2025-03-02T04:34:46.431Z"}, {"duration": 1010, "start_time": "2025-03-02T04:34:46.935Z"}, {"duration": 87, "start_time": "2025-03-02T04:34:47.947Z"}, {"duration": 442, "start_time": "2025-03-02T04:34:48.038Z"}, {"duration": 2765, "start_time": "2025-03-02T20:24:29.488Z"}, {"duration": 560, "start_time": "2025-03-02T20:24:32.255Z"}, {"duration": 1573, "start_time": "2025-03-02T20:24:32.818Z"}, {"duration": 740, "start_time": "2025-03-02T20:24:34.394Z"}, {"duration": 1009, "start_time": "2025-03-02T20:28:16.034Z"}, {"duration": 497, "start_time": "2025-03-02T20:28:17.045Z"}, {"duration": 1360, "start_time": "2025-03-02T20:28:17.543Z"}, {"duration": 747, "start_time": "2025-03-02T20:28:18.905Z"}, {"duration": 1025, "start_time": "2025-03-02T20:31:26.386Z"}, {"duration": 489, "start_time": "2025-03-02T20:31:27.414Z"}, {"duration": 1437, "start_time": "2025-03-02T20:31:27.905Z"}, {"duration": 733, "start_time": "2025-03-02T20:31:29.344Z"}, {"duration": 1035, "start_time": "2025-03-02T20:35:20.320Z"}, {"duration": 503, "start_time": "2025-03-02T20:35:21.358Z"}, {"duration": 1399, "start_time": "2025-03-02T20:35:21.863Z"}, {"duration": 755, "start_time": "2025-03-02T20:35:23.264Z"}, {"duration": 409659, "start_time": "2025-03-02T20:35:24.022Z"}], "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.19"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": true, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}